{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "data": {
      "text/plain": "   Gender  Height  Weight  Index\n0    Male     174      96      4\n1    Male     189      87      2\n2  Female     185     110      4\n3  Female     195     104      3\n4    Male     149      61      3",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>Height</th>\n      <th>Weight</th>\n      <th>Index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Male</td>\n      <td>174</td>\n      <td>96</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Male</td>\n      <td>189</td>\n      <td>87</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Female</td>\n      <td>185</td>\n      <td>110</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Female</td>\n      <td>195</td>\n      <td>104</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Male</td>\n      <td>149</td>\n      <td>61</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"data/500_Person_Gender_Height_Weight_Index.csv\")\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imagine that we want to predict whether or not the person is obese. According to the information of the dataset, people with an index of 4 or 5 are obese, so we could create a variable that reflects this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "data": {
      "text/plain": "   Gender  Height  Weight  obese\n0    Male     174      96      1\n1    Male     189      87      0\n2  Female     185     110      1\n3  Female     195     104      0\n4    Male     149      61      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>Height</th>\n      <th>Weight</th>\n      <th>obese</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Male</td>\n      <td>174</td>\n      <td>96</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Male</td>\n      <td>189</td>\n      <td>87</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Female</td>\n      <td>185</td>\n      <td>110</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Female</td>\n      <td>195</td>\n      <td>104</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Male</td>\n      <td>149</td>\n      <td>61</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['obese'] = (data.Index >= 4).astype('int')\n",
    "data.drop('Index', axis=1, inplace=True)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then define two impurity metric based on Gini index and entropy:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4998\n",
      "0.9997114388674198\n"
     ]
    }
   ],
   "source": [
    "def gini_impurity(y):\n",
    "    '''\n",
    "    Given a Pandas Series, it calculates the Gini Impurity.\n",
    "    y: variable with which calculate Gini Impurity.\n",
    "    '''\n",
    "    if isinstance(y, pd.Series):\n",
    "        # TODO: calculate gini_impurity\n",
    "        return gini\n",
    "\n",
    "    else:\n",
    "        raise ('Object must be a Pandas Series.')\n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    '''\n",
    "    Given a Pandas Series, it calculates the entropy.\n",
    "    y: variable with which calculate entropy.\n",
    "    '''\n",
    "    if isinstance(y, pd.Series):\n",
    "        a = y.value_counts() / y.shape[0]\n",
    "        entropy = np.sum(-a * np.log2(a + 1e-9))\n",
    "        return (entropy)\n",
    "\n",
    "    else:\n",
    "        raise ('Object must be a Pandas Series.')\n",
    "\n",
    "\n",
    "print(gini_impurity(data.Gender))\n",
    "print(entropy(data.Gender))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results above simply implies that 'Gender' can provide almost no information about obesity. This can be described more accurately by measuring information gain (IG), which is defined as follows:\n",
    "$$\n",
    "IG=E(d)-\\sum _s \\frac{|s|}{|d|} E(s)\n",
    "$$\n",
    "That is, the information gain is the difference between **overall entropy** and the **weighted sum of the entropy of each cut group**. Implement this function in the following:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0005506911187600494"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def information_gain(y, mask, func=entropy):\n",
    "    '''\n",
    "    It returns the Information Gain of a variable given a loss function.\n",
    "    y: target variable.\n",
    "    mask: split choice.\n",
    "    func: function to be used to calculate Information Gain in case os classification.\n",
    "    '''\n",
    "\n",
    "    a = sum(mask)\n",
    "    b = mask.shape[0] - a\n",
    "\n",
    "    if (a == 0 or b == 0):\n",
    "        ig = 0\n",
    "    else:\n",
    "        ig = # TODO: calculate the information gain according to the equation above\n",
    "\n",
    "    return ig\n",
    "\n",
    "\n",
    "information_gain(data['obese'], data['Gender'] == 'Male')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can see more clearly that knowing the gender provide little information about obesity. The next question is, how do we select variable for cutting and what is the proper value of cutting? In essence, we achieve this by iterating all possible choices:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def max_information_gain_split(x, y, func=entropy):\n",
    "    '''\n",
    "    Given a predictor & target variable, returns the best split, the error and the type of variable based on a selected cost function.\n",
    "    x: predictor variable as Pandas Series.\n",
    "    y: target variable as Pandas Series.\n",
    "    func: function to be used to calculate the best split.\n",
    "    '''\n",
    "\n",
    "    split_value = []\n",
    "    ig = []\n",
    "\n",
    "    numeric_variable = True if x.dtypes != 'O' else False\n",
    "\n",
    "    options = x.sort_values().unique()[1:]\n",
    "\n",
    "    # Calculate ig for all values\n",
    "    for val in options:\n",
    "        # TODO: calculate information gain when spliting by val\n",
    "        ig.append(val_ig)\n",
    "        split_value.append(val)\n",
    "\n",
    "    # Check if there are more than 1 results if not, return False\n",
    "    if len(ig) == 0:\n",
    "        return (None, None, None, False)\n",
    "\n",
    "    else:\n",
    "        # Get results with highest IG\n",
    "        best_ig = max(ig)\n",
    "        best_ig_index = ig.index(best_ig)\n",
    "        best_split = split_value[best_ig_index]\n",
    "        return (best_ig, best_split, numeric_variable, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After having this function, we are able to determine 1) which variable to make a split and 2) how to make the best split:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "     Gender    Height    Weight\n0  0.000551  0.064748  0.382454\n1    [Male]       174       103\n2     False      True      True\n3      True      True      True",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>Height</th>\n      <th>Weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000551</td>\n      <td>0.064748</td>\n      <td>0.382454</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[Male]</td>\n      <td>174</td>\n      <td>103</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop('obese', axis=1).apply(max_information_gain_split, y=data['obese'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output suggests that we should split by weight first, where the best split can provide us with an information gain of 0.382454."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we are ready to train our decision tree. Several hyperparameters need to be determined to prevent the tree from growing too much, thus avoiding overfitting:\n",
    "\n",
    "- max_depth: maximum depth of the tree. If we set it to None, the tree will grow until all the leaves are pure or the hyperparameter min_samples_split has been reached.\n",
    "- min_samples_split: indicates the minimum number of observations a sheet must have to continue creating new nodes.\n",
    "- min_information_gain: the minimum amount the Information Gain must increase for the tree to continue growing\n",
    "\n",
    "Then, to create the tree, we need to:\n",
    "\n",
    "- Make sure that the conditions established by min_samples_split and max_depth are being fulfilled.\n",
    "- Make the split.\n",
    "- Ensure that min_information_gain if fulfilled.\n",
    "- Save the data of the split and repeat the process."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "def get_best_split(y, data):\n",
    "    masks = data.drop(y, axis=1).apply(max_information_gain_split, y=data[y])\n",
    "    if sum(masks.iloc[3, :]) == 0:\n",
    "        return (None, None, None, None)\n",
    "\n",
    "    else:\n",
    "        # Get only masks that can be splitted\n",
    "        masks = masks.loc[:, masks.iloc[3, :]]\n",
    "\n",
    "        # Get the results for split with highest IG\n",
    "        split_variable = masks.iloc[0].astype(np.float32).idxmax()\n",
    "        # split_valid = masks[split_variable][]\n",
    "        split_value = masks[split_variable].iloc[1]\n",
    "        split_ig = masks[split_variable].iloc[0]\n",
    "        split_numeric = masks[split_variable].iloc[2]\n",
    "\n",
    "        return (split_variable, split_value, split_ig, split_numeric)\n",
    "\n",
    "\n",
    "def make_split(variable, value, data, is_numeric):\n",
    "    if is_numeric:\n",
    "        data_1 = data[data[variable] < value]\n",
    "        data_2 = data[(data[variable] < value) == False]\n",
    "\n",
    "    else:\n",
    "        data_1 = data[data[variable].isin(value)]\n",
    "        data_2 = data[(data[variable].isin(value)) == False]\n",
    "\n",
    "    return (data_1, data_2)\n",
    "\n",
    "\n",
    "def make_prediction(data):\n",
    "    pred = data.value_counts().idxmax()\n",
    "    return pred\n",
    "\n",
    "\n",
    "def train_tree(data, y, max_depth=None, min_samples_split=None, min_information_gain=1e-20, counter=0,\n",
    "               max_categories=20):\n",
    "    '''\n",
    "    Trains a Decission Tree\n",
    "    data: Data to be used to train the Decission Tree\n",
    "    y: target variable column name\n",
    "    max_depth: maximum depth to stop splitting.\n",
    "    min_samples_split: minimum number of observations to make a split.\n",
    "    min_information_gain: minimum ig gain to consider a split to be valid.\n",
    "    max_categories: maximum number of different values accepted for categorical values. High number of values will slow down learning process. R\n",
    "    '''\n",
    "\n",
    "    # Check that max_categories is fulfilled\n",
    "    if counter == 0:\n",
    "        types = data.dtypes\n",
    "        check_columns = types[types == \"object\"].index\n",
    "        for column in check_columns:\n",
    "            var_length = len(data[column].value_counts())\n",
    "            if var_length > max_categories:\n",
    "                raise ValueError('The variable ' + column + ' has ' + str(\n",
    "                    var_length) + ' unique values, which is more than the accepted ones: ' + str(max_categories))\n",
    "\n",
    "    # Check for depth conditions\n",
    "    if max_depth == None:\n",
    "        depth_cond = True\n",
    "\n",
    "    else:\n",
    "        if counter < max_depth:\n",
    "            depth_cond = True\n",
    "\n",
    "        else:\n",
    "            depth_cond = False\n",
    "\n",
    "    # Check for sample conditions\n",
    "    if min_samples_split == None:\n",
    "        sample_cond = True\n",
    "\n",
    "    else:\n",
    "        if data.shape[0] > min_samples_split:\n",
    "            sample_cond = True\n",
    "\n",
    "        else:\n",
    "            sample_cond = False\n",
    "\n",
    "    # Check for ig condition\n",
    "    if depth_cond & sample_cond:\n",
    "\n",
    "        var, val, ig, var_type = get_best_split(y, data)\n",
    "\n",
    "        # If ig condition is fulfilled, make split\n",
    "        if ig is not None and ig >= min_information_gain:\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            left, right = make_split(var, val, data, var_type)\n",
    "\n",
    "            # Instantiate sub-tree\n",
    "            split_type = \"<=\" if var_type else \"in\"\n",
    "            question = \"{} {}  {}\".format(var, split_type, val)\n",
    "            # question = \"\\n\" + counter*\" \" + \"|->\" + var + \" \" + split_type + \" \" + str(val)\n",
    "            subtree = {question: []}\n",
    "\n",
    "            # Find answers (recursion)\n",
    "            yes_answer = train_tree(left, y, max_depth, min_samples_split, min_information_gain, counter)\n",
    "\n",
    "            no_answer = train_tree(right, y, max_depth, min_samples_split, min_information_gain, counter)\n",
    "\n",
    "            if yes_answer == no_answer:\n",
    "                subtree = yes_answer\n",
    "\n",
    "            else:\n",
    "                subtree[question].append(yes_answer)\n",
    "                subtree[question].append(no_answer)\n",
    "\n",
    "        # If it doesn't match IG condition, make prediction\n",
    "        else:\n",
    "            pred = make_prediction(data[y])\n",
    "            return pred\n",
    "\n",
    "    # Drop dataset if doesn't match depth or sample conditions\n",
    "    else:\n",
    "        pred = make_prediction(data[y])\n",
    "        return pred\n",
    "\n",
    "    return subtree"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can train our tree by setting proper hyperparameters and train it on the training set:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [],
   "source": [
    "max_depth = 5\n",
    "min_samples_split = 20\n",
    "min_information_gain = 1e-5\n",
    "\n",
    "num_train_data = 400\n",
    "train_data, val_data = data.iloc[:num_train_data], data.iloc[num_train_data:]\n",
    "\n",
    "decisions = train_tree(train_data, 'obese', max_depth, min_samples_split, min_information_gain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The decision tree is:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [
    {
     "data": {
      "text/plain": "{'Weight <=  103': [{'Height <=  178': [{'Weight <=  74': [0,\n      {'Height <=  162': [1, {'Weight <=  92': [0, 1]}]}]},\n    0]},\n  {'Weight <=  116': [{'Height <=  189': [{'Weight <=  115': [1, 0]}, 0]},\n    1]}]}"
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The decision is formatted as a nested container with dict and list. Each decision is a dict with decision as key and a list as value. The list has only two possibilities: another decision or a direct answer (0 or 1, meaning obese or not).\n",
    "Maybe this description is still obscure, let's classify a real data sample based on our decision rule!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determine: Weight <=  103\n",
      "observation 87 less than 103.0\n",
      "determine: Height <=  178\n",
      "observation 189 greater than 178.0\n",
      "decision-tree result: 0\n"
     ]
    }
   ],
   "source": [
    "def clasificar_datos(observacion, rule, verbose=False):\n",
    "    question = list(rule.keys())[0]\n",
    "    if verbose:\n",
    "        print(f'determine: {question}')\n",
    "\n",
    "    if question.split()[1] == '<=':\n",
    "\n",
    "        if observacion[question.split()[0]] <= float(question.split()[2]):\n",
    "            if verbose:\n",
    "                print(f'observation {observacion[question.split()[0]]} less than {float(question.split()[2])}')\n",
    "            answer = rule[question][0]\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f'observation {observacion[question.split()[0]]} greater than {float(question.split()[2])}')\n",
    "            answer = rule[question][1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        if observacion[question.split()[0]] in (question.split()[2]):\n",
    "            answer = rule[question][0]\n",
    "        else:\n",
    "            answer = rule[question][1]\n",
    "\n",
    "    # If the answer is not a dictionary\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return clasificar_datos(observacion, answer, verbose=verbose)\n",
    "\n",
    "\n",
    "print('decision-tree result:', clasificar_datos(observacion=data.iloc[1], rule=decisions, verbose=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Therefore, by consecutive decisions, our algorithm finally decides that the sample at index 1 is not obese.\n",
    "Finally, we can calculate the accuracy of our algorithm as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of this model is 0.97\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i, row in val_data.iterrows():\n",
    "    if (row.obese and clasificar_datos(row, decisions)) or (not row.obese and not clasificar_datos(row, decisions)):\n",
    "        correct += 1\n",
    "print(f'Accuracy of this model is {correct / len(val_data)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}